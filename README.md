# Explainable Adaptation to Concept Drift in Decision Tree Ensembles

Decision trees are widely used in machine learning due to their interpretability.
  However, when the underlying data distribution changes---a phenomenon known as concept drift---their performance can degrade significantly.
  Retraining the model from scratch discards the learned structure and offers no insight into which nodes were actually affected.
  Prior work, APPETITE, used spectrum-based fault localization to identify and modify a single faulty node, but cannot handle drift that affects multiple nodes simultaneously or that demands structural changes to the tree.
  We propose BROAD (Breadth tRee Oriented Adaptation for Drift), which formalizes the Decision Tree Concept Drift Problem as a Model-Based Diagnosis task and employs BARINEL, a Bayesian algorithm that ranks multi-node fault hypotheses across the entire tree.
  For repair, BROAD applies \subfitfull, which replaces affected subtrees with new ones fitted to the post-drift distribution.
  BROAD relies on lightweight node-level statistics and requires no access to the original training data.
  Experiments show that BROAD outperforms APPETITE both in terms of recovery performance as well as the explanation. In addition, BROAD achieves accuracy recovery comparable to full retraining, which, unlike BROAD, provides no insight into the nature of the drift and requires access to the original data.

## This Repository is listed as:
- APPETITE - The BROAD algorithm, with baselines in it.
- Tester - All the files relevant to evaluate the algorithm, including configurations for each baseline diagnoser that ran.
- Data - The datasets with their descriptions.
- appendix (duplicated as results) - The results for both the drift scenario runs (experinent1=Feature-Shift, experiment2=Distribution-Edge).

## Reproduce:
To run the algorithms, all needed is:
1. Install the required packages using [the requirements file](https://github.com/gitanonym/BROAD/blob/main/requirements.txt).
2. Choose your desired experiment [in the tester-constatns file by DRIFT_SYNTHESIZING_VERSION value](https://github.com/gitanonym/DAPCROP/blob/main/code/Tester/TesterConstants.py#L18) (1 for experiment1, 2 for experiment2).
3. Run test_all.py file with -d 1 argument: 
    ```
        python -m test_all.py -d 1
    ```

That's it. The results will be saved in the results folder.


[Appendix of all the datasets and corresponding results can be located here](https://github.com/gitanonym/BROAD/tree/main/appendix/)
    Good Luck!
